{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ae0f85-f44e-483a-a97e-d29271e12115",
   "metadata": {
    "id": "49ae0f85-f44e-483a-a97e-d29271e12115"
   },
   "source": [
    "# Lecture 8 - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64e078-e002-483e-b74d-d22a3fc95260",
   "metadata": {
    "id": "9c64e078-e002-483e-b74d-d22a3fc95260"
   },
   "source": [
    "Recurrent neural networks can handle time series data of different lengths. In this demo notebook we will first look deeper into Deep Knowledge Tracing, before showing examples of different types of neural network models for tracing and time series tasks. The learning objectives of this notebook are as follows:\n",
    "\n",
    "1. Explore the differences between deep learning architectures for time-series data with LSTMs, GRUs and RNNs.\n",
    "\n",
    "2. Implement hyperparameter tuning for a deep learning pipeline.\n",
    "\n",
    "3. Contrast two behavioral time-series data settings: a model that makes a prediction at every time interval vs. a model that makes an overall prediction at the end of the time series.\n",
    "\n",
    "If you are using EPFL's Noto, this notebook will need to use the `tensorflow` kernel for the dependencies to be installed appropriately. Change the kernel in the upper right corner of Noto. Select `tensorflow`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46040b51-cf1f-4029-a2a6-2bf7c94ee04e",
   "metadata": {
    "id": "46040b51-cf1f-4029-a2a6-2bf7c94ee04e"
   },
   "outputs": [],
   "source": [
    "# Load standard imports for the rest of the notebook.\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import tensorflow as tf\n",
    "\n",
    "# In this demo, we use a lot of SciKit-Learn functions, as imported below.\n",
    "from sklearn import feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "n0TVeNWyGrMR",
   "metadata": {
    "id": "n0TVeNWyGrMR"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./../../data/\"\n",
    "\n",
    "# Setting this variable to true will train the DKT model fitting, evaluation and\n",
    "# hyperparameter tuning from scratch, which will take ~1 hour on Colab. \n",
    "train_from_scratch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173b722b-20ec-4ff5-8d07-821b0ad3eb2c",
   "metadata": {
    "id": "173b722b-20ec-4ff5-8d07-821b0ad3eb2c"
   },
   "outputs": [],
   "source": [
    "def create_iterator(data):\n",
    "    '''\n",
    "    Create an iterator to split interactions in data into train and test, with the same student not appearing in two diverse folds.\n",
    "    :param data:        Dataframe with student's interactions.\n",
    "    :return:            An iterator.\n",
    "    '''    \n",
    "    # Both passing a matrix with the raw data or just an array of indexes works\n",
    "    X = np.arange(len(data.index))\n",
    "    # Groups of interactions are identified by the user id (we do not want the same user appearing in two folds)\n",
    "    groups = data['user_id'].values \n",
    "    return model_selection.GroupShuffleSplit(n_splits=1, train_size=.8, test_size=0.2, random_state=0).split(X, groups=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea03ccc-da16-4b98-b6b4-b2dd4decdce3",
   "metadata": {
    "id": "2ea03ccc-da16-4b98-b6b4-b2dd4decdce3",
    "tags": []
   },
   "source": [
    "## Deep Knowledge Tracing (DKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7509d1-0a9c-45f7-9fa2-7954e6013644",
   "metadata": {
    "id": "2a7509d1-0a9c-45f7-9fa2-7954e6013644"
   },
   "source": [
    "We begin by loading the data of the ASSISTments dataset (that we have explored in previous lectures). \n",
    "\n",
    "The ASSISTments data sets are often used for benchmarking knowledge tracing models. We will play with a simplified data set that contains the following columns:\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| user_id | The ID of the student who is solving the problem.  | |\n",
    "| order_id | The temporal ID (timestamp) associated with the student's answer to the problem.  | |\n",
    "| skill_name | The name of the skill associated with the problem. | |\n",
    "| correct | The student's performance on the problem: 1 if the problem's answer is correct at the first attempt, 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b61453-4bb2-4f39-acc0-b63b7c4e11e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a4b61453-4bb2-4f39-acc0-b63b7c4e11e2",
    "outputId": "d4978055-318b-4345-c63a-b10f2123ebb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>skill_name</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64525</td>\n",
       "      <td>33022537</td>\n",
       "      <td>Box and Whisker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64525</td>\n",
       "      <td>33022709</td>\n",
       "      <td>Box and Whisker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70363</td>\n",
       "      <td>35450204</td>\n",
       "      <td>Box and Whisker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70363</td>\n",
       "      <td>35450295</td>\n",
       "      <td>Box and Whisker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70363</td>\n",
       "      <td>35450311</td>\n",
       "      <td>Box and Whisker</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  order_id       skill_name  correct\n",
       "0    64525  33022537  Box and Whisker        1\n",
       "1    64525  33022709  Box and Whisker        1\n",
       "2    70363  35450204  Box and Whisker        0\n",
       "3    70363  35450295  Box and Whisker        1\n",
       "4    70363  35450311  Box and Whisker        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_DIR + 'assistments.csv', low_memory=False).dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5d4b6-08d3-486d-8a48-474f5419c6d2",
   "metadata": {
    "id": "7eb5d4b6-08d3-486d-8a48-474f5419c6d2"
   },
   "source": [
    "Next, we print the number of students and skills in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e14881-5cd4-4a50-b98c-fa0fa5a92626",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5e14881-5cd4-4a50-b98c-fa0fa5a92626",
    "outputId": "57e9f6da-bb5e-4f55-8bf6-218016df8148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique students in the dataset: 4151\n",
      "Number of unique skills in the dataset: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique students in the dataset:\", len(set(data['user_id'])))\n",
    "print(\"Number of unique skills in the dataset:\", len(set(data['skill_name'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KMG2F0_b5F6P",
   "metadata": {
    "id": "KMG2F0_b5F6P"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03374938-2408-42e8-a39a-130b049c9614",
   "metadata": {
    "id": "03374938-2408-42e8-a39a-130b049c9614"
   },
   "source": [
    "Since the data needs to be fed into the model in batches, we need to specify in advance how many elements per batch the DKT model will receive. DKT also requires that all sequences need to be of the same length in order to be used as model input. \n",
    "\n",
    "Given that students have different number of opportunities across skills, we need to define a scheme such that the sequences will be the same length. We choose to pad our values to the maximum sequence length and determine a masking value (for the model to ignore) for those entries that are introduced as a padding into the student's sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cPQYLdJ91Pgd",
   "metadata": {
    "id": "cPQYLdJ91Pgd"
   },
   "outputs": [],
   "source": [
    "def prepare_seq(df):\n",
    "    '''\n",
    "    Extract user_id sequence in preparation for DKT. The output of this function \n",
    "    feeds into the prepare_data() function. \n",
    "    '''\n",
    "    # Enumerate skill id as a categorical variable \n",
    "    # (i.e. [32, 12, 32, 45] -> [0, 1, 0, 2])\n",
    "    df['skill'], skill_codes = pd.factorize(df['skill_name'], sort=True)\n",
    "\n",
    "    # Cross skill id with answer to form a synthetic feature\n",
    "    df['skill_with_answer'] = df['skill'] * 2 + df['correct']\n",
    "\n",
    "    # Convert to a sequence per user_id and shift features 1 timestep\n",
    "    seq = df.groupby('user_id').apply(lambda r: (r['skill_with_answer'].values[:-1], r['skill'].values[1:], r['correct'].values[1:],))\n",
    "    \n",
    "    # Get max skill depth and max feature depth\n",
    "    skill_depth = df['skill'].max() \n",
    "    features_depth = df['skill_with_answer'].max() + 1\n",
    "\n",
    "    return seq, features_depth, skill_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "U-hvy9zC1Q42",
   "metadata": {
    "id": "U-hvy9zC1Q42"
   },
   "outputs": [],
   "source": [
    "def prepare_data(seq, params, features_depth, skill_depth):\n",
    "    '''\n",
    "    Manipulate the data sequences into the right format for DKT with padding by batch\n",
    "    and encoding categorical features.\n",
    "    '''\n",
    "    \n",
    "    # Get Tensorflow Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generator=lambda: seq, output_types=(tf.int32, tf.int32, tf.float32))\n",
    "\n",
    "    # Encode categorical features and merge skills with labels to compute target loss\n",
    "    dataset = dataset.map(\n",
    "        lambda feat, skill, label: (\n",
    "            tf.one_hot(feat, depth=features_depth),\n",
    "            tf.concat(values=[tf.one_hot(skill, depth=skill_depth), tf.expand_dims(label, -1)], axis=-1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Pad sequences to the appropriate length per batch\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=params['batch_size'],\n",
    "        padding_values=(params['mask_value'], params['mask_value']),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "\n",
    "    return dataset.repeat(), len(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S8hbOm2GdFAu",
   "metadata": {
    "id": "S8hbOm2GdFAu"
   },
   "source": [
    "#### Your Turn (In-Class Discussion)\n",
    "What do these hyperparameters mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "xdlT_vrl2hc0",
   "metadata": {
    "id": "xdlT_vrl2hc0"
   },
   "outputs": [],
   "source": [
    "# Specify the model hyperparameters. Full descriptions included in the demo notebook!\n",
    "params = {}\n",
    "\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 16\n",
    "params['epochs'] = 20\n",
    "params['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc0dd7-e714-4086-b55d-ee2111573d57",
   "metadata": {
    "id": "4cbc0dd7-e714-4086-b55d-ee2111573d57"
   },
   "source": [
    "We then split the data into a train, a validation and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc81f84e-ec62-4442-95d5-58a24f428c28",
   "metadata": {
    "id": "cc81f84e-ec62-4442-95d5-58a24f428c28"
   },
   "outputs": [],
   "source": [
    "# Obtain indexes for training and test sets\n",
    "train_index, test_index = next(create_iterator(data))\n",
    "\n",
    "# Split the data into training and test\n",
    "X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "# Obtain indexes for training and validation sets\n",
    "train_val_index, val_index = next(create_iterator(X_train))\n",
    "\n",
    "# Split the training data into training and validation\n",
    "X_train_val, X_val = X_train.iloc[train_val_index], X_train.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "EsLMWu6G2sIk",
   "metadata": {
    "id": "EsLMWu6G2sIk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 18:59:51.688341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-29 18:59:51.688412: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-29 18:59:51.688457: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (noto.epfl.ch): /proc/driver/nvidia/version does not exist\n",
      "2023-06-29 18:59:51.689086: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build TensorFlow sequence datasets for training, validation, and test data\n",
    "seq, features_depth, skill_depth = prepare_seq(data)\n",
    "seq_train = seq[X_train_val.user_id.unique()]\n",
    "seq_val = seq[X_val.user_id.unique()]\n",
    "seq_test = seq[X_test.user_id.unique()]\n",
    "\n",
    "# Prepare the training, validation, and test data in the DKT input format\n",
    "tf_train, length = prepare_data(seq_train, params, features_depth, skill_depth)\n",
    "tf_val, val_length  = prepare_data(seq_val, params, features_depth, skill_depth)\n",
    "tf_test, test_length = prepare_data(seq_test, params, features_depth, skill_depth)\n",
    "\n",
    "# Calculate the length of each of the train-test-val sets and store as parameters\n",
    "params['train_size'] = int(length // params['batch_size'])\n",
    "params['val_size'] = int(val_length // params['batch_size'])\n",
    "params['test_size'] = int(test_length // params['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xV_6sL3j5JqO",
   "metadata": {
    "id": "xV_6sL3j5JqO"
   },
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504b631-a4f5-45b9-8e4f-37203b1e84b2",
   "metadata": {
    "id": "c504b631-a4f5-45b9-8e4f-37203b1e84b2"
   },
   "source": [
    "First, we train DKT using an LSTM architecture and default parameter settings. We use a validation set to monitor prediction accuracy of the model and store the model with the best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44168260-1ff8-47d7-9476-c1bf6fd9ffe7",
   "metadata": {
    "id": "44168260-1ff8-47d7-9476-c1bf6fd9ffe7"
   },
   "source": [
    "Considering that we padded the sequences such that all have the same length, we need to remove predictions for the time steps that are based on padded data. To this end, we implement a function called get_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae15693f-2019-4838-a656-67e3f191e6c3",
   "metadata": {
    "id": "ae15693f-2019-4838-a656-67e3f191e6c3"
   },
   "outputs": [],
   "source": [
    "def get_target(y_true, y_pred, mask_value=params['mask_value']):\n",
    "    ''' \n",
    "    Adjust y_true and y_pred to ignore predictions made using padded values.\n",
    "    '''\n",
    "    # Get skills and labels from y_true\n",
    "    mask = 1. - tf.cast(tf.equal(y_true, mask_value), y_true.dtype)\n",
    "    y_true = y_true * mask\n",
    "\n",
    "    skills, y_true = tf.split(y_true, num_or_size_splits=[-1, 1], axis=-1)\n",
    "\n",
    "    # Get predictions for each skill\n",
    "    y_pred = tf.reduce_sum(y_pred * skills, axis=-1, keepdims=True)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc0eb8-4dfc-411e-a53f-fa0d3d836aaf",
   "metadata": {
    "id": "88cc0eb8-4dfc-411e-a53f-fa0d3d836aaf"
   },
   "source": [
    "While training and evaluating the model, we will monitor the following performance metrics. Please, note that we need to process our targets before using the default TensorFlow metric functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38f90b9-82f5-42eb-b732-027551086674",
   "metadata": {
    "id": "f38f90b9-82f5-42eb-b732-027551086674"
   },
   "outputs": [],
   "source": [
    "class AUC(tf.keras.metrics.AUC):\n",
    "    # Our custom AUC calls our get_target function first to remove predictions on padded values, \n",
    "    # then computes a standard AUC metric.\n",
    "    def __init__(self):\n",
    "        # We use a super constructor here just to make our metric name pretty!\n",
    "        super(AUC, self).__init__(name='auc')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(AUC, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "class RMSE(tf.keras.metrics.RootMeanSquaredError):\n",
    "    # Our custom RMSE calls our get_target function first to remove predictions on padded values, \n",
    "    # then computes a standard RMSE metric.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(RMSE, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "        \n",
    "def CustomBinaryCrossEntropy(y_true, y_pred): \n",
    "    # Our custom binary cross entropy loss calls our get_target function first \n",
    "    # to remove predictions on padded values, then computes standard binary cross-entropy.\n",
    "    y_true, y_pred = get_target(y_true, y_pred)\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73112110-b257-49f1-a443-0746658714e9",
   "metadata": {
    "id": "73112110-b257-49f1-a443-0746658714e9"
   },
   "source": [
    "We define an LSTM and a GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce3ed83-1c11-479c-9467-2d6894c49c22",
   "metadata": {
    "id": "5ce3ed83-1c11-479c-9467-2d6894c49c22"
   },
   "outputs": [],
   "source": [
    "def create_model_lstm(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create an LSTM model architecture\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features), name='inputs')\n",
    "\n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], return_sequences=True, dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "    # We use a dense layer with the sigmoid function activation to map our predictions \n",
    "    # between 0 and 1.\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation='sigmoid')\n",
    "\n",
    "    # The TimeDistributed layer takes the dense layer predictions and applies the sigmoid \n",
    "    # activation function to all time steps.\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=CustomBinaryCrossEntropy, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[AUC(), RMSE()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our DKT model using an LSTM\n",
    "dkt_lstm = create_model_lstm(features_depth, skill_depth, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec7c14fd-8d4a-4b20-b5c1-2587335ebe70",
   "metadata": {
    "id": "ec7c14fd-8d4a-4b20-b5c1-2587335ebe70"
   },
   "outputs": [],
   "source": [
    "def create_model_gru(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create a GRU model architecture\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features), name='inputs')\n",
    "\n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This GRU layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.GRU(params['recurrent_units'], return_sequences=True, dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "    # We use a dense layer with the sigmoid function activation to map our predictions \n",
    "    # between 0 and 1.\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation='sigmoid')\n",
    "\n",
    "    # The TimeDistributed layer takes the dense layer predictions and applies the sigmoid \n",
    "    # activation function to all time steps.\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=CustomBinaryCrossEntropy, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[AUC(), RMSE()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our DKT model using a GRU\n",
    "dkt_gru = create_model_gru(features_depth, skill_depth, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BDF1YnZ77bGR",
   "metadata": {
    "id": "BDF1YnZ77bGR"
   },
   "source": [
    "### Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380dd28-4fe5-42e4-a32f-a21736fcdf70",
   "metadata": {
    "id": "c380dd28-4fe5-42e4-a32f-a21736fcdf70"
   },
   "source": [
    "Next we train the models and then evaluate them on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "649014c0-0a71-43ca-83c7-1b525c19f3d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "649014c0-0a71-43ca-83c7-1b525c19f3d9",
    "outputId": "6a0a6042-216b-480e-f0b1-e1c7b7dfbaf2"
   },
   "outputs": [],
   "source": [
    "# This cell takes 8 minutes to run. On default, we will not run the training experiments below.\n",
    "# However, if you would like to run it from scratch, you can modify train_from_scratch=True \n",
    "# at the beginning of the notebook.\n",
    "\n",
    "if train_from_scratch:\n",
    "  # This line tells our training procedure to only save the best version of the model at any given time.\n",
    "  ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                    save_best_only=True, save_weights_only=True)\n",
    "\n",
    "  # Let's fit our LSTM model on the training data. This cell takes 8 minutes to run.\n",
    "  history = dkt_lstm.fit(tf_train, epochs=params['epochs'], steps_per_epoch=params['train_size']-1, \n",
    "                        validation_data=tf_val, validation_steps=params['val_size'],\n",
    "                        callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "AFWPgmhpvul7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFWPgmhpvul7",
    "outputId": "0415713b-1afd-440a-84dc-d2b7e99da172"
   },
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "  # We load the LSTM model with the best performance, and evaluate it on the test set. \n",
    "  dkt_lstm.load_weights(params['best_model_weights'])\n",
    "  dkt_lstm.evaluate(tf_test, steps=params['test_size'], verbose=params['verbose'], return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d224ac3-0c5b-4824-90ed-c345b798fcbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d224ac3-0c5b-4824-90ed-c345b798fcbf",
    "outputId": "bb8c54ca-28c5-4f6b-ab45-35828f445729"
   },
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "  # This line tells our training procedure to only save the best version of the model at any given time.\n",
    "  ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                    save_best_only=True, save_weights_only=True)\n",
    "\n",
    "  # Let's fit our GRU model on the training data. This cell takes 8 minutes to run.\n",
    "  history = dkt_gru.fit(tf_train, epochs=params['epochs'], steps_per_epoch=params['train_size']-1, \n",
    "                        validation_data=tf_val, validation_steps=params['val_size'],\n",
    "                        callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vWkpS2fzC0MS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWkpS2fzC0MS",
    "outputId": "3d681c01-d5cb-47ce-c962-6d2c39e41861"
   },
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "  # We load the GRU model with the best performance, and evaluate it on the test set. \n",
    "  dkt_gru.load_weights(params['best_model_weights'])\n",
    "  dkt_gru.evaluate(tf_test, steps=params['test_size'], verbose=params['verbose'], return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd0d25-7ac3-4e0e-883f-3a35893d065e",
   "metadata": {
    "id": "adcd0d25-7ac3-4e0e-883f-3a35893d065e"
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "As we have seen, we need to specify a lot of hyperparameters. In a next step, we perform a small grid search for the number of recurrent units in the LSTM: {8, 16, 32, 64}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6YRkJNteF7q0",
   "metadata": {
    "id": "6YRkJNteF7q0"
   },
   "outputs": [],
   "source": [
    "# Modify the dictionary of parameters so that each parameter maps to a list of possibilities.\n",
    "# In this case, we're only searching over the recurrent_units and leaving the rest of the \n",
    "# parameters fixed to their default values.\n",
    "params_space = {param: [value] for param, value in params.items()}\n",
    "params_space['recurrent_units'] = [8, 16, 32, 64]\n",
    "params_grid = ParameterGrid(params_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cVQLRMVgGRce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVQLRMVgGRce",
    "outputId": "7737f5ed-ee26-45c1-8a44-f26a0117087f"
   },
   "outputs": [],
   "source": [
    "# For each combination of candidate parameters, fit a model on the training set \n",
    "# and evaluate it on the validation set (as we've seen in Lecture 5). \n",
    "\n",
    "# NOTE: This cell will take 40 minutes to run from scratch.\n",
    "if train_from_scratch:\n",
    "  results = {}\n",
    "\n",
    "  # For each parameter setting in the grid search of parameters\n",
    "  for params_i in params_grid:\n",
    "\n",
    "      # Create a LSTM model with the specific parameter setting params_i\n",
    "      dkt_lstm = create_model_lstm(features_depth, skill_depth, params_i)\n",
    "\n",
    "      save_model_name = params_i['best_model_weights'] + str(params_i['recurrent_units'])\n",
    "\n",
    "      # Save the best version of the model through the training epochs\n",
    "      ckp_callback = tf.keras.callbacks.ModelCheckpoint(save_model_name, \n",
    "                                                        save_best_only=True, save_weights_only=True)\n",
    "\n",
    "      # Fit the model on the training data with the appropriate parameters  \n",
    "      dkt_lstm.fit(tf_train, \n",
    "                  epochs=params_i['epochs'],\n",
    "                  steps_per_epoch=params_i['train_size']-1, \n",
    "                  validation_data=tf_val, \n",
    "                  validation_steps=params_i['val_size'],\n",
    "                  callbacks=[ckp_callback],\n",
    "                  verbose=params_i['verbose'])\n",
    "\n",
    "      # Evaluate the model performance\n",
    "      results[params_i['recurrent_units']] = dkt_lstm.evaluate(tf_val,\n",
    "                                                              steps=params_i['val_size'], \n",
    "                                                              verbose=params_i['verbose'], \n",
    "                                                              return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42YW9E5hH7vo",
   "metadata": {
    "id": "42YW9E5hH7vo"
   },
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "  # Sort candidate parameters according to their accuracy\n",
    "  results = sorted(results.items(), key=lambda x: x[1]['auc'], reverse=True)\n",
    "\n",
    "  # Obtain the best parameters\n",
    "  best_params = results[0][0]\n",
    "  best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "V4PLC5CPIDVK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4PLC5CPIDVK",
    "outputId": "982fc038-e275-41b6-dc11-7499d749cc48"
   },
   "outputs": [],
   "source": [
    "if train_from_scratch:\n",
    "  # Load the best model variant from the hyperparameter gridsearch\n",
    "  dkt_lstm.load_weights(params['best_model_weights'] + str(best_params))\n",
    "  dkt_lstm.evaluate(tf_test, steps=params['test_size'], \n",
    "                        verbose=params['verbose'], \n",
    "                        return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4efd2-67ca-4c54-b1b9-195a18ee6845",
   "metadata": {
    "id": "53c4efd2-67ca-4c54-b1b9-195a18ee6845"
   },
   "source": [
    "## Tracing and Time-Series Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881d886-8f14-42c2-87cb-a964fc057fa6",
   "metadata": {
    "id": "0881d886-8f14-42c2-87cb-a964fc057fa6"
   },
   "source": [
    "Next, we perform experiments with recurrent neural networks for tracing as well as the time series task. We first load the data for the tracing task. It stems from a massive open online course (MOOC) hosted by EPFL. We first load the features as well as the labels to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da3c8271-1074-4ec7-a0f9-44862b35b02f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da3c8271-1074-4ec7-a0f9-44862b35b02f",
    "outputId": "69ba13db-bfaf-4525-edfe-4c282362a3e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'week', 'TotalClicksVideoLoad', 'AvgWatchedWeeklyProp',\n",
       "       'StdWatchedWeeklyProp', 'AvgReplayedWeeklyProp',\n",
       "       'StdReplayedWeeklyProp', 'AvgInterruptedWeeklyProp',\n",
       "       'StdInterruptedWeeklyProp', 'TotalClicksVideoConati',\n",
       "       'FrequencyEventVideo', 'FrequencyEventLoad', 'FrequencyEventVideoPlay',\n",
       "       'FrequencyEventVideoPause', 'FrequencyEventVideoStop',\n",
       "       'FrequencyEventVideoSeekBackward', 'FrequencyEventVideoSeekForward',\n",
       "       'FrequencyEventVideoSpeedChange', 'AvgSeekLength', 'StdSeekLength',\n",
       "       'AvgPauseDuration', 'StdPauseDuration', 'AvgTimeSpeedingUp',\n",
       "       'StdTimeSpeedingUp', 'RegPeakTimeDayHour', 'RegPeriodicityM1',\n",
       "       'DelayLecture', 'TotalClicks', 'NumberOfSessions', 'TotalTimeSessions',\n",
       "       'AvgTimeSessions', 'StdTimeBetweenSessions', 'StdTimeSessions',\n",
       "       'TotalClicksWeekday', 'TotalClicksWeekend', 'RatioClicksWeekendDay',\n",
       "       'TotalClicksVideoChen', 'TotalClicksProblem', 'TotalTimeProblem',\n",
       "       'TotalTimeVideo', 'CompetencyAlignment', 'CompetencyAnticipation',\n",
       "       'ContentAlignment', 'ContentAnticipation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mooc_feat = pd.read_csv(DATA_DIR + 'mooc_feat.csv', low_memory=False)\n",
    "mooc_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d3073e1-c78b-4929-874f-581426a4e37a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4d3073e1-c78b-4929-874f-581426a4e37a",
    "outputId": "077e360d-5e4e-4ebe-96fe-747ff87e8b84"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>week</th>\n",
       "      <th>quiz_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1593</td>\n",
       "      <td>0</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1593</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1593</td>\n",
       "      <td>2</td>\n",
       "      <td>0.807141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1593</td>\n",
       "      <td>3</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1593</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59685</th>\n",
       "      <td>3353959</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59686</th>\n",
       "      <td>3353959</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59687</th>\n",
       "      <td>3353959</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59688</th>\n",
       "      <td>3353959</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59689</th>\n",
       "      <td>3353959</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59690 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  week  quiz_correct\n",
       "0         1593     0      0.929825\n",
       "1         1593     1           NaN\n",
       "2         1593     2      0.807141\n",
       "3         1593     3      0.960000\n",
       "4         1593     4      0.900000\n",
       "...        ...   ...           ...\n",
       "59685  3353959     5           NaN\n",
       "59686  3353959     6           NaN\n",
       "59687  3353959     7           NaN\n",
       "59688  3353959     8           NaN\n",
       "59689  3353959     9           NaN\n",
       "\n",
       "[59690 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mooc_quizzes = pd.read_csv(DATA_DIR + 'mooc_quizzes.csv', low_memory=False)\n",
    "display(mooc_quizzes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IXbziXoUC25k",
   "metadata": {
    "id": "IXbziXoUC25k"
   },
   "source": [
    "### Tracing: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "vcuUz1niMQNp",
   "metadata": {
    "id": "vcuUz1niMQNp"
   },
   "outputs": [],
   "source": [
    "# Normalize all the features with min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "mooc_feat.iloc[:, 2:] = scaler.fit_transform(mooc_feat.iloc[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c911f5ff-e2a8-43fa-983b-d0dc0137e41c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c911f5ff-e2a8-43fa-983b-d0dc0137e41c",
    "outputId": "be5a1472-a88e-4b47-d264-63a5496b4996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique students in the dataset: 4352\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique students in the dataset:\", len(set(mooc_feat['user_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec606bd-cf1d-4a48-ad49-2b4848d1bd6f",
   "metadata": {
    "id": "dec606bd-cf1d-4a48-ad49-2b4848d1bd6f"
   },
   "source": [
    "In this analysis, we want to predict **weekly quiz performance** of the students. We perform the following preprocessing steps to prepare our data:\n",
    "- First, we observe from the data frame mooc_quizzes that quite a number of students have not solved quizzes in all weeks. We will use a mask to ignore weeks for students with missing quiz answers. We create a new data frame df_y (the outcome), where we replace NaNs (for quiz_correct) with -1. We also create a data frame df_x, where we replace the according input feature values with -1.\n",
    "\n",
    "- Second, we bring df_y and df_x to an appropriate shape.\n",
    "\n",
    "  df_y should become a NumPy array of size:\n",
    "\n",
    "  `size(df_y) = num_of_students * num_of_weeks`\n",
    "\n",
    "  df_x should become a NumPy array of size:\n",
    "\n",
    "  `size(df_x) = num_of_students * num_of_weeks * num_of_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zQNXIR62F9YQ",
   "metadata": {
    "id": "zQNXIR62F9YQ"
   },
   "source": [
    "We create a data frame `df_x`, where we ignore weeks for students with missing quiz answers by filling in the appropriate feature values with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hIDieWdrB8GI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "hIDieWdrB8GI",
    "outputId": "e8447911-cf0e-473d-df57-bb6a692cfe6f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Array conditional must be same shape as self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m mask \u001b[38;5;241m=\u001b[39m mooc_quizzes\u001b[38;5;241m.\u001b[39mquiz_correct\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39mzeros((mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], num_index), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m), \n\u001b[1;32m      7\u001b[0m                        mask[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(num_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m df_x \u001b[38;5;241m=\u001b[39m \u001b[43mmooc_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df_x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:10976\u001b[0m, in \u001b[0;36mDataFrame.mask\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m  10963\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[1;32m  10964\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m  10965\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10974\u001b[0m     try_cast\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m  10975\u001b[0m ):\n\u001b[0;32m> 10976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_cast\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:9346\u001b[0m, in \u001b[0;36mNDFrame.mask\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   9343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cond, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__invert__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   9344\u001b[0m     cond \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cond)\n\u001b[0;32m-> 9346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9349\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9350\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9352\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:10961\u001b[0m, in \u001b[0;36mDataFrame.where\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m  10948\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[1;32m  10949\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m  10950\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10959\u001b[0m     try_cast\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m  10960\u001b[0m ):\n\u001b[0;32m> 10961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_cast\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:9310\u001b[0m, in \u001b[0;36mNDFrame.where\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   9302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_cast \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   9303\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   9304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry_cast keyword is deprecated and will be removed in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   9305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   9306\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   9307\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   9308\u001b[0m     )\n\u001b[0;32m-> 9310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:9054\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[0;34m(self, cond, other, inplace, axis, level, errors)\u001b[0m\n\u001b[1;32m   9052\u001b[0m         cond \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(cond)\n\u001b[1;32m   9053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cond\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m-> 9054\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray conditional must be same shape as self\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9055\u001b[0m     cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(cond, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_axes_dict())\n\u001b[1;32m   9057\u001b[0m \u001b[38;5;66;03m# make sure we are boolean\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Array conditional must be same shape as self"
     ]
    }
   ],
   "source": [
    "num_features = 42\n",
    "num_index = mooc_feat.shape[1] - num_features\n",
    "\n",
    "# Mask df_x values\n",
    "mask = mooc_quizzes.quiz_correct.isna().values\n",
    "mask = np.concatenate([np.zeros((mask.shape[0], num_index), dtype=bool), \n",
    "                       mask[:, None].repeat(num_features, axis=1)], axis=1)\n",
    "df_x = mooc_feat.mask(mask, -1)\n",
    "df_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlBnu7VJoeho",
   "metadata": {
    "id": "tlBnu7VJoeho"
   },
   "source": [
    "We create `df_y` (the outcome), where we replace NaNs (for quiz_correct) with -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67kEAmPDDJhr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "67kEAmPDDJhr",
    "outputId": "7fb6371d-8cf5-4e0d-df7d-7bcf645f01ea"
   },
   "outputs": [],
   "source": [
    "df_y = mooc_quizzes.fillna(-1)\n",
    "df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4wvShWLF0Ef",
   "metadata": {
    "id": "c4wvShWLF0Ef"
   },
   "source": [
    "We bring `df_y` and `df_x` to an appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rr-moZnw-Lrk",
   "metadata": {
    "id": "Rr-moZnw-Lrk"
   },
   "outputs": [],
   "source": [
    "num_weeks = df_y.week.nunique()\n",
    "df_y = df_y.quiz_correct.values.reshape(-1, num_weeks, 1)\n",
    "df_x = df_x.iloc[:, num_index:].values.reshape(-1, num_weeks, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc044cb-4d28-4a89-9a54-f27def90b0bc",
   "metadata": {
    "id": "cbc044cb-4d28-4a89-9a54-f27def90b0bc",
    "tags": []
   },
   "source": [
    "We then split the data into train, test, and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8KclC5ufblXt",
   "metadata": {
    "id": "8KclC5ufblXt"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split the MOOC data into training and test sets.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df_x_train, df_x_test, df_y_train, df_y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 3\u001b[0m                                                         \u001b[43mdf_x\u001b[49m, df_y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                                         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Split the training dataset into validation and training sets.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df_x_train_val, df_x_val, df_y_train_val, df_y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m      8\u001b[0m                                                         df_x_train, df_y_train, \n\u001b[1;32m      9\u001b[0m                                                         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the MOOC data into training and test sets.\n",
    "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(\n",
    "                                                        df_x, df_y, test_size=0.2, \n",
    "                                                        random_state=0)\n",
    "\n",
    "# Split the training dataset into validation and training sets.\n",
    "df_x_train_val, df_x_val, df_y_train_val, df_y_val = train_test_split(\n",
    "                                                        df_x_train, df_y_train, \n",
    "                                                        test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GRAUH96UC570",
   "metadata": {
    "id": "GRAUH96UC570"
   },
   "source": [
    "### Tracing: Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c71de6-1bea-4d8d-b273-33418b430a7f",
   "metadata": {
    "id": "a5c71de6-1bea-4d8d-b273-33418b430a7f"
   },
   "source": [
    "Next, we build an LSTM model for predicting student performance on the MOOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ADS8GgE5CEUi",
   "metadata": {
    "id": "ADS8GgE5CEUi"
   },
   "outputs": [],
   "source": [
    "# We use the default hyperparameters, as described in detail in the DKT model creation section.\n",
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1 # Verbose = {0,1,2}\n",
    "params['best_model_weights'] = 'weights/bestmodel' # File to save the model\n",
    "params['optimizer'] = 'adam' # Optimizer to use\n",
    "params['recurrent_units'] = 32 # Number of RNN units\n",
    "params['epochs'] = 20 # Number of epochs to train\n",
    "params['dropout_rate'] = 0.1 # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7SsVwF5XCZew",
   "metadata": {
    "id": "7SsVwF5XCZew"
   },
   "outputs": [],
   "source": [
    "def create_model_lstm_MOOC(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create an LSTM model architecture.\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features), name='inputs')\n",
    "    \n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], \n",
    "                             return_sequences=True, \n",
    "                             dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "        \n",
    "    # We use a dense layer with the linear function activation to map our predictions \n",
    "    # on a linear scale. Note that this has changed from a sigmoid activated dense layer\n",
    "    # in the previous LSTM function.\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation='linear')\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=tf.keras.losses.MSE, \n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "dkt_lstm = create_model_lstm_MOOC(num_features, 1, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158gxKmhEfhb",
   "metadata": {
    "id": "158gxKmhEfhb"
   },
   "source": [
    "### Tracing: Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5LghGCVBEhtY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LghGCVBEhtY",
    "outputId": "070dd45e-d14a-4fad-fc0f-8a0100a127d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 7s 22ms/step - loss: 0.0360 - root_mean_squared_error: 0.3023 - val_loss: 0.0201 - val_root_mean_squared_error: 0.2244\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0197 - root_mean_squared_error: 0.2238 - val_loss: 0.0179 - val_root_mean_squared_error: 0.2114\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0184 - root_mean_squared_error: 0.2161 - val_loss: 0.0175 - val_root_mean_squared_error: 0.2092\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0180 - root_mean_squared_error: 0.2137 - val_loss: 0.0176 - val_root_mean_squared_error: 0.2099\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0180 - root_mean_squared_error: 0.2141 - val_loss: 0.0171 - val_root_mean_squared_error: 0.2070\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0176 - root_mean_squared_error: 0.2112 - val_loss: 0.0170 - val_root_mean_squared_error: 0.2059\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0177 - root_mean_squared_error: 0.2118 - val_loss: 0.0169 - val_root_mean_squared_error: 0.2056\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0176 - root_mean_squared_error: 0.2113 - val_loss: 0.0175 - val_root_mean_squared_error: 0.2089\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0174 - root_mean_squared_error: 0.2105 - val_loss: 0.0168 - val_root_mean_squared_error: 0.2051\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0173 - root_mean_squared_error: 0.2098 - val_loss: 0.0168 - val_root_mean_squared_error: 0.2048\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0173 - root_mean_squared_error: 0.2098 - val_loss: 0.0167 - val_root_mean_squared_error: 0.2046\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0174 - root_mean_squared_error: 0.2102 - val_loss: 0.0167 - val_root_mean_squared_error: 0.2041\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0172 - root_mean_squared_error: 0.2089 - val_loss: 0.0166 - val_root_mean_squared_error: 0.2038\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0174 - root_mean_squared_error: 0.2099 - val_loss: 0.0179 - val_root_mean_squared_error: 0.2118\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0173 - root_mean_squared_error: 0.2098 - val_loss: 0.0166 - val_root_mean_squared_error: 0.2039\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0171 - root_mean_squared_error: 0.2083 - val_loss: 0.0166 - val_root_mean_squared_error: 0.2040\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0172 - root_mean_squared_error: 0.2092 - val_loss: 0.0168 - val_root_mean_squared_error: 0.2052\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0171 - root_mean_squared_error: 0.2085 - val_loss: 0.0164 - val_root_mean_squared_error: 0.2027\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0170 - root_mean_squared_error: 0.2080 - val_loss: 0.0166 - val_root_mean_squared_error: 0.2036\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.0170 - root_mean_squared_error: 0.2080 - val_loss: 0.0164 - val_root_mean_squared_error: 0.2027\n"
     ]
    }
   ],
   "source": [
    "# This model takes less than 5 minutes to train on Noto (< 1 minute on Colab).\n",
    "\n",
    "# We save only the best model during the training process.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Fit the DKT LSTM on DSP1 data.\n",
    "history = dkt_lstm.fit(df_x_train_val, df_y_train_val, epochs=params['epochs'],\n",
    "                       validation_data=(df_x_val, df_y_val),\n",
    "                       callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sPd2YM1zZEkS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPd2YM1zZEkS",
    "outputId": "9e22761b-9ae7-44e0-ccc3-42282e7ec6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0172 - root_mean_squared_error: 0.2052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.017214568331837654, 'root_mean_squared_error': 0.2052297443151474}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best performing model and evaluate the performance.\n",
    "dkt_lstm.load_weights(params['best_model_weights'])\n",
    "dkt_lstm.evaluate(df_x_test, df_y_test, verbose=params['verbose'], return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ijMQ-uFbqA8W",
   "metadata": {
    "id": "ijMQ-uFbqA8W"
   },
   "source": [
    "### Time Series: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db459c-421b-4138-a75c-293a1212cf51",
   "metadata": {
    "id": "98db459c-421b-4138-a75c-293a1212cf51"
   },
   "source": [
    "We can modify our model to predict after `n` weeks whether students will pass or fail the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbeb8375-7270-4393-a2ec-5eef816e146f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cbeb8375-7270-4393-a2ec-5eef816e146f",
    "outputId": "3c74a543-18e2-4eb0-859f-d8f30145fdfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d8251501-fb29-4ed7-a2ed-e24f528cd934\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>label-pass-fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1836</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8251501-fb29-4ed7-a2ed-e24f528cd934')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d8251501-fb29-4ed7-a2ed-e24f528cd934 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d8251501-fb29-4ed7-a2ed-e24f528cd934');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   user_id  label-pass-fail\n",
       "0     1593              0.0\n",
       "1     1626              1.0\n",
       "2     1787              1.0\n",
       "3     1824              1.0\n",
       "4     1836              1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mooc_labels = pd.read_csv(DATA_DIR + 'mooc_lab.csv', low_memory=False).dropna()\n",
    "mooc_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288ce05-4d7c-4566-a695-fd6a91de9ed2",
   "metadata": {
    "id": "f288ce05-4d7c-4566-a695-fd6a91de9ed2"
   },
   "source": [
    "We choose `n = 5` weeks and therefore drop all the data from weeks 5 through 10. Since this problem refers to early peformance prediction, we can only train on weeks 1 through 4 of student data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "UDDn8n7DaH0a",
   "metadata": {
    "id": "UDDn8n7DaH0a"
   },
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552f101-a01c-4a69-ae13-78248ad7ba97",
   "metadata": {
    "id": "f552f101-a01c-4a69-ae13-78248ad7ba97"
   },
   "source": [
    "We preprocess our data for this task: \n",
    "\n",
    "- `mooc_labels` should become a NumPy array of size `num_of_students`.\n",
    "\n",
    "- `df_x` should become a NumPy array of size `num_of_students * n * num_of_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2RjEoy0aaKeP",
   "metadata": {
    "id": "2RjEoy0aaKeP"
   },
   "outputs": [],
   "source": [
    "df_x_binary = df_x[:, :n, :]\n",
    "df_y_binary = mooc_labels['label-pass-fail'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b88330-af08-48fb-9587-6fc87cd34cbb",
   "metadata": {
    "id": "43b88330-af08-48fb-9587-6fc87cd34cbb"
   },
   "source": [
    "Finally, we split the data into train/validation/test sets. We do a stratified split (on label-pass-fail) so that the classes are representatively balanced across each of our dataset divisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "_AcIVWvbcvbQ",
   "metadata": {
    "id": "_AcIVWvbcvbQ"
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets.\n",
    "df_x_binary_train, df_x_binary_test, df_y_binary_train, df_y_binary_test = train_test_split(\n",
    "                                                                            df_x_binary, \n",
    "                                                                            df_y_binary,\n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=0, \n",
    "                                                                            stratify=df_y_binary)\n",
    "\n",
    "# Split training into training and validation sets.\n",
    "df_x_binary_train_val, df_x_binary_val, df_y_binary_train_val, df_y_binary_val = train_test_split(\n",
    "                                                                            df_x_binary_train, \n",
    "                                                                            df_y_binary_train, \n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=0, \n",
    "                                                                            stratify=df_y_binary_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d967a-9039-4b08-9106-b24b6ba15cc8",
   "metadata": {
    "id": "013d967a-9039-4b08-9106-b24b6ba15cc8"
   },
   "source": [
    "### Time Series: Model Creation\n",
    "\n",
    "Now, we can again create an lstm model, which takes the features up to week 5 as an input and predicts the pass/fail label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OgPMmEinf8Gq",
   "metadata": {
    "id": "OgPMmEinf8Gq"
   },
   "source": [
    "#### Your Turn (Code)\n",
    "\n",
    "Fill in the create_model function for time-series prediction using an LSTM below. You can refer to the DKT task and the above tracing task for example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d7b50-a9bf-4a69-aa85-4c0733d64c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_lstm_mooc_binary(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create an LSTM model architecture.\n",
    "    inputs = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, \n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=[tf.keras.metrics.AUC(), 'binary_accuracy'])    \n",
    "    return model\n",
    "\n",
    "time_series_lstm = create_model_lstm_mooc_binary(num_features, 1, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fEaSNCn_1d91",
   "metadata": {
    "id": "fEaSNCn_1d91"
   },
   "source": [
    "### Time Series: Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rgwXrLNnen4c",
   "metadata": {
    "id": "rgwXrLNnen4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This model should take ~30 seconds to train.\n",
    "\n",
    "# We save only the best model during the training process.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Fit the DKT LSTM on DSP1 data.\n",
    "history = time_series_lstm.fit(df_x_binary_train_val, \n",
    "                               df_y_binary_train_val, \n",
    "                               epochs=params['epochs'],\n",
    "                               validation_data=(df_x_binary_val, df_y_binary_val),\n",
    "                               callbacks=[ckp_callback], \n",
    "                               verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e1269-f0e0-4916-a3dd-fa996e5e53d8",
   "metadata": {
    "id": "6b1e1269-f0e0-4916-a3dd-fa996e5e53d8"
   },
   "source": [
    "To evaluate performance of the model, we can also use `predict` instead of `evaluate` to get the actual predictions of the model. We can then compute any evaluation metric based on the true labels and the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O03_Uk_KemAA",
   "metadata": {
    "id": "O03_Uk_KemAA"
   },
   "outputs": [],
   "source": [
    "# Load the best version of the the trained model and evaluate its performance on the test_set.\n",
    "time_series_lstm.load_weights(params['best_model_weights'])\n",
    "predictions = time_series_lstm.predict(df_x_binary_test)\n",
    "bac = balanced_accuracy_score(df_y_binary_test, predictions>0.5)\n",
    "auc = roc_auc_score(df_y_binary_test,predictions)\n",
    "print(\"Balanced accuracy: \", bac)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xWcOYLm_hRpf",
   "metadata": {
    "id": "xWcOYLm_hRpf"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://courdier.pythonanywhere.com/get-send-code\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'lecture-08',\n",
    "    'session_owner': 'mlbd',\n",
    "    'sender_name': input(\"Your name: \"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvegjopLgTwu",
   "metadata": {
    "id": "cvegjopLgTwu"
   },
   "outputs": [],
   "source": [
    "### Share the bac with us\n",
    "bac_time_series = bac\n",
    "send(bac_time_series, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lI5N75Zrobn",
   "metadata": {
    "id": "3lI5N75Zrobn"
   },
   "source": [
    "### Time Series: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uzbuLvhbgcaB",
   "metadata": {
    "id": "uzbuLvhbgcaB"
   },
   "source": [
    "#### Your Turn (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2NLKnbRzf_P0",
   "metadata": {
    "id": "2NLKnbRzf_P0"
   },
   "outputs": [],
   "source": [
    "# Modify the dictionary of parameters so that each parameter maps to a list of possibilities.\n",
    "# You can tune any hyperparameter that you want. We advice to stay with a small grid...\n",
    "params_space = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1aEPNQTgEdI",
   "metadata": {
    "id": "h1aEPNQTgEdI"
   },
   "outputs": [],
   "source": [
    "# Conduct the gridsearch over hyperparameters.\n",
    "# This cell should take ~3 minutes to run.\n",
    "results = {}\n",
    "\n",
    "# For each parameter setting in the grid search of parameters\n",
    "for params_i in params_grid:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NgrodvSKgHvT",
   "metadata": {
    "id": "NgrodvSKgHvT"
   },
   "outputs": [],
   "source": [
    "# Sort candidate parameters according to their accuracy\n",
    "results = sorted(results.items(), key=lambda x: x[1]['binary_accuracy'], reverse=True)\n",
    "\n",
    "# Obtain the best parameters\n",
    "best_params = results[0][0]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g_Tkk2kjgU_L",
   "metadata": {
    "id": "g_Tkk2kjgU_L"
   },
   "outputs": [],
   "source": [
    "# Load the best model variant from the hyperparameter gridsearch\n",
    "time_series_lstm.load_weights(params['best_model_weights'] + str(best_params))\n",
    "predictions = time_series_lstm.predict(df_x_binary_test)\n",
    "bac = balanced_accuracy_score(df_y_binary_test, predictions>0.5)\n",
    "auc = roc_auc_score(df_y_binary_test,predictions)\n",
    "print(\"Balanced accuracy: \", bac)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WcfazAbBZh0S",
   "metadata": {
    "id": "WcfazAbBZh0S"
   },
   "outputs": [],
   "source": [
    "### Share the bac with us\n",
    "bac_hyperparam_tuning = bac\n",
    "send(bac_hyperparam_tuning, 2) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "08-student-version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
